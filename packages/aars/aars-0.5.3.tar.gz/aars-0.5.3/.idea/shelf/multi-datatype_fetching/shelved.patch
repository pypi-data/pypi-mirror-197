Index: src/aars/core.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import asyncio\nimport math\nimport warnings\nfrom abc import ABC\nfrom collections import OrderedDict\nfrom operator import attrgetter\nfrom typing import Type, TypeVar, Dict, ClassVar, List, Set, Any, Union, Tuple, Optional, Generic, AsyncIterator\n\nimport aiohttp\nfrom aiohttp import ServerDisconnectedError\nfrom aleph_client.vm.cache import VmCache\nfrom aleph_message.models import PostMessage, BaseMessage\nfrom pydantic import BaseModel\n\nimport aleph_client.asynchronous as client\nfrom aleph_client.types import Account\nfrom aleph_client.chains.ethereum import get_fallback_account\nfrom aleph_client.conf import settings\n\nfrom .utils import subslices, async_iterator_to_list\nfrom .exceptions import AlreadyForgottenError\n\nT = TypeVar('T', bound='Record')\n\n\nclass Record(BaseModel, ABC):\n    \"\"\"\n    A basic record which is persisted on Aleph decentralized storage.\n\n    Records can be updated: revision numbers begin at 0 (original upload) and increment for each `upsert()` call.\n\n    Previous revisions can be restored by calling `fetch_revision(rev_no=<number>)` or `fetch_revision(\n    rev_hash=<item_hash of inserted update>)`.\n\n    They can also be forgotten: Aleph will ask the network to forget given item, in order to allow for GDPR-compliant\n    applications.\n\n    Records have an `indices` class attribute, which allows one to select an index and query it with a key.\n    \"\"\"\n    forgotten: bool = False\n    id_hash: Optional[str] = None\n    current_revision: Optional[int] = None\n    revision_hashes: List[str] = []\n    __indices: ClassVar[Dict[str, 'Index']] = {}\n\n    def __repr__(self):\n        return f'{type(self).__name__}({self.id_hash})'\n\n    def __str__(self):\n        return f'{type(self).__name__} {self.__dict__}'\n\n    @property\n    def content(self) -> Dict[str, Any]:\n        \"\"\"\n        :return: content dictionary of the object, as it is to be stored on Aleph.\n        \"\"\"\n        return self.dict(exclude={'id_hash', 'current_revision', 'revision_hashes', 'forgotten'})\n\n    async def update_revision_hashes(self: T):\n        \"\"\"\n        Updates the list of available revision hashes, in order to fetch these.\n        \"\"\"\n        assert self.id_hash is not None\n        self.revision_hashes = [self.id_hash] + await async_iterator_to_list(AARS.fetch_revisions(type(self), ref=self.id_hash))\n        if self.current_revision is None:\n            # latest revision\n            self.current_revision = len(self.revision_hashes) - 1\n\n    async def fetch_revision(self: T, rev_no: Optional[int] = None, rev_hash: Optional[str] = None) -> T:\n        \"\"\"\n        Fetches a revision of the object by revision number (0 => original) or revision hash.\n        :param rev_no: the revision number of the revision to fetch.\n        :param rev_hash: the hash of the revision to fetch.\n        \"\"\"\n        assert self.id_hash is not None, 'Cannot fetch revision of an object which has not been posted yet.'\n        assert self.current_revision is not None\n\n        if rev_no is not None:\n            if rev_no < 0:\n                rev_no = len(self.revision_hashes) + rev_no\n            if self.current_revision == rev_no:\n                return self\n            elif rev_no > len(self.revision_hashes):\n                raise IndexError(f'No revision no. {rev_no} found for {self}')\n            else:\n                self.current_revision = rev_no\n        elif rev_hash is not None:\n            if rev_hash == self.revision_hashes[self.current_revision]:\n                return self\n            try:\n                self.current_revision = self.revision_hashes.index(rev_hash)\n            except ValueError:\n                raise IndexError(f'{rev_hash} is not a revision of {self}')\n        else:\n            raise ValueError('Either rev or hash must be provided')\n\n        resp = await AARS.fetch_exact(\n            type(self),\n            self.revision_hashes[self.current_revision]\n        )\n        self.__dict__.update(resp.content)\n\n        return self\n\n    async def upsert(self):\n        \"\"\"\n        Posts a new item to Aleph or amends it, if it was already posted. Will add new items to local indices.\n        For indices to be persisted on Aleph, you need to call `upsert()` on the index itself or `cls.update_indices()`.\n        \"\"\"\n        await AARS.post_or_amend_object(self)\n        if self.current_revision == 0:\n            [index.add_record(self) for index in self.get_indices()]\n        return self\n\n    async def forget(self):\n        \"\"\"\n        Orders Aleph to forget a specific object with all its revisions.\n        The forgotten object should be deleted afterward, as it is useless now.\n        \"\"\"\n        if not self.forgotten:\n            await AARS.forget_objects([self])\n            self.forgotten = True\n        else:\n            raise AlreadyForgottenError(self)\n\n    @classmethod\n    async def create(cls: Type[T], **kwargs) -> T:\n        \"\"\"\n        Initializes and uploads a new item with given properties.\n        \"\"\"\n        obj = cls(**kwargs)\n        return await obj.upsert()\n\n    @classmethod\n    async def from_post(cls: Type[T], post: PostMessage) -> T:\n        \"\"\"\n        Initializes a record object from its PostMessage.\n        :param post: the PostMessage to initialize from.\n        \"\"\"\n        obj = cls(**post.content.content)\n        if post.content.ref is None:\n            obj.id_hash = post.item_hash\n        else:\n            obj.id_hash = post.content.ref\n        await obj.update_revision_hashes()\n        assert obj.id_hash is not None\n        obj.current_revision = obj.revision_hashes.index(obj.id_hash)\n        return obj\n\n    @classmethod\n    async def from_dict(cls: Type[T], post: Dict[str, Any]) -> T:\n        \"\"\"\n        Initializes a record object from its raw Aleph data.\n        :post: Raw Aleph data.\n        \"\"\"\n        obj = cls(**post['content'])\n        if post.get('ref') is None:\n            obj.id_hash = post['item_hash']\n        else:\n            obj.id_hash = post['ref']\n        await obj.update_revision_hashes()\n        assert obj.id_hash is not None\n        obj.current_revision = obj.revision_hashes.index(obj.id_hash)\n        return obj\n\n    @classmethod\n    async def fetch(cls: Type[T], hashes: Union[str, List[str]]) -> List[T]:\n        \"\"\"\n        Fetches one or more objects of given type by its/their item_hash[es].\n        \"\"\"\n        if not isinstance(hashes, List):\n            hashes = [hashes]\n        return await async_iterator_to_list(AARS.fetch_records(cls, list(hashes)))\n\n    @classmethod\n    async def fetch_all(cls: Type[T]) -> List[T]:\n        \"\"\"\n        Fetches all objects of given type.\n        \"\"\"\n        return await async_iterator_to_list(AARS.fetch_records(cls))\n\n    @classmethod\n    async def query(cls: Type[T], **kwargs) -> List[T]:\n        \"\"\"\n        Queries an object by given properties through an index, in order to fetch applicable records.\n        An index name is defined as '<object_class>.[<object_properties>.]' and is initialized by creating\n        an Index instance, targeting a BaseRecord class with a list of properties.\n\n        >>> Index(MyRecord, ['property1', 'property2'])\n\n        This will create an index named 'MyRecord.property1.property2' which can be queried with:\n\n        >>> MyRecord.query(property1='value1', property2='value2')\n\n        If no index is defined for the given properties, an IndexError is raised.\n\n        If only a part of the keys is indexed for the given query, a fallback index is used and locally filtered.\n        \"\"\"\n        sorted_items: OrderedDict[str, Any] = OrderedDict(sorted(kwargs.items()))\n        sorted_keys = sorted_items.keys()\n        full_index_name = cls.__name__ + '.' + '.'.join(sorted_keys)\n        index = cls.get_index(full_index_name)\n        keys = repr(index).split('.')[1:]\n        items = await index.lookup(\n            OrderedDict({key: str(sorted_items.get(key)) for key in keys})\n        )\n        if full_index_name != repr(index):\n            filtered_items = list()\n            for item in items:\n                # eliminate the item which does not fulfill this properties\n                class_properties = vars(item)\n                required_class_properties = {key: class_properties.get(key) for key in sorted_keys}\n                if required_class_properties == dict(sorted_items):\n                    filtered_items.append(item)\n            return filtered_items\n        else:\n            return items\n\n    @classmethod\n    def add_index(cls: Type[T], index: 'Index') -> None:\n        cls.__indices[repr(index)] = index\n\n    @classmethod\n    def get_index(cls: Type[T], index_name: str) -> 'Index[T]':\n        \"\"\"\n        Returns an index or any of its subindices by its name. The name is defined as\n        '<object_class>.[<object_properties>.]' with the properties being sorted alphabetically. For example,\n        Book.author.title is a valid index name, while Book.title.author is not.\n        :param index_name: The name of the index to fetch.\n        :return: The index instance or a subindex.\n        \"\"\"\n        index = cls.__indices.get(index_name)\n        if index is None:\n            key_subslices = subslices(list(index_name.split('.')[1:]))\n            # returns all plausible combinations of keys\n            key_subslices = sorted(key_subslices, key=lambda x: len(x), reverse=True)\n            for keys in key_subslices:\n                name = cls.__name__ + '.' + '.'.join(keys)\n                if cls.__indices.get(name):\n                    warnings.warn(f'No index {index_name} found. Using {name} instead.')\n                    return cls.__indices[name]\n            raise IndexError(f'No index or subindex for {index_name} found.')\n        return index\n\n    @classmethod\n    def get_indices(cls: Type[T]) -> List['Index']:\n        if cls == Record:\n            return list(cls.__indices.values())\n        return [index for index in cls.__indices.values() if index.datatype == cls]\n\n    @classmethod\n    async def update_indices(cls: Type[T]) -> None:\n        \"\"\"Updates all indices of given type.\"\"\"\n        tasks = [index.upsert() for index in cls.get_indices()]\n        await asyncio.gather(*tasks)\n\n    @classmethod\n    async def regenerate_indices(cls: Type[T]) -> None:\n        \"\"\"Regenerates all indices of given type.\"\"\"\n        items = await cls.fetch_all()\n        for index in cls.get_indices():\n            index.regenerate(items)\n\n\nclass Index(Record, Generic[T]):\n    \"\"\"\n    Class to define Indices.\n    \"\"\"\n    datatype: Type[T]\n    index_on: List[str]\n    hashmap: Dict[Union[str, Tuple], str] = {}\n\n    def __init__(self, datatype: Type[T], on: Union[str, List[str], Tuple[str]]):\n        \"\"\"\n        Creates a new index given a datatype and a single or multiple properties to index on.\n\n        >>> Index(MyRecord, 'foo')\n\n        This will create an index named 'MyRecord.foo', which is stored in the `MyRecord` class.\n        It is not recommended using the index directly, but rather through the `query` method of the `Record` class like\n        so:\n\n        >>> MyRecord.query(foo='bar')\n\n        This returns all records of type MyRecord where foo is equal to 'bar'.\n\n        :param datatype: The datatype to index.\n        :param on: The properties to index on.\n        \"\"\"\n        if isinstance(on, str):\n            on = [on]\n        super(Index, self).__init__(datatype=datatype, index_on=sorted(on))\n        datatype.add_index(self)\n\n    def __str__(self):\n        return f\"Index({self.datatype.__name__}.{'.'.join(self.index_on)})\"\n\n    def __repr__(self):\n        return f\"{self.datatype.__name__}.{'.'.join(self.index_on)}\"\n\n    async def lookup(self, keys: Optional[Union[OrderedDict[str, str], List[OrderedDict[str, str]]]] = None) -> List[T]:\n        \"\"\"\n        Fetches records with given values for the indexed properties.\n\n        :param keys: The hash(es) to fetch.\n        \"\"\"\n        hashes: Set[Optional[str]]\n        if keys is None:\n            hashes = set(self.hashmap.values())\n        elif isinstance(keys, OrderedDict):\n            if len(keys.values()) == 1:\n                hashes = {self.hashmap.get(list(keys.values())[0])}\n            else:\n                # noinspection PySetFunctionToLiteral\n                hashes = set([self.hashmap.get(tuple(keys.values())), ])\n        elif isinstance(keys, List):\n            hashes = set([self.hashmap.get(tuple(key.values())) for key in keys])\n        else:\n            hashes = set()\n\n        return await async_iterator_to_list(AARS.fetch_records(self.datatype, list([h for h in hashes if h is not None])))\n\n    def add_record(self, obj: T):\n        \"\"\"Adds a record to the index.\"\"\"\n        assert issubclass(type(obj), Record)\n        assert obj.id_hash is not None\n        self.hashmap[attrgetter(*self.index_on)(obj)] = obj.id_hash\n\n    def regenerate(self, items: List[T]):\n        \"\"\"Regenerates the index with given items.\"\"\"\n        self.hashmap = {}\n        for item in items:\n            self.add_record(item)\n\n\nclass AARS:\n    account: Account\n    channel: str\n    api_url: str\n    retry_count: int\n    session: Optional[aiohttp.ClientSession]\n    cache: Optional[VmCache]\n\n    def __init__(self,\n                 account: Optional[Account] = None,\n                 channel: Optional[str] = None,\n                 api_url: Optional[str] = None,\n                 session: Optional[aiohttp.ClientSession] = None,\n                 cache: Optional[VmCache] = None,\n                 retry_count: Optional[int] = None):\n        \"\"\"\n        Initializes the SDK with an account and a channel.\n        :param cache: Whether to use Aleph VM caching when running AARS code.\n        :param account: Account with which to sign the messages.\n        :param channel: Channel to which to send the messages.\n        :param api_url: The API URL to use. Defaults to an official Aleph API host.\n        :param session: An aiohttp session to use. Defaults to a new session.\n        \"\"\"\n        AARS.account = account if account else get_fallback_account()\n        AARS.channel = channel if channel else 'AARS_TEST'\n        AARS.api_url = api_url if api_url else settings.API_HOST\n        AARS.session = session if session else None\n        AARS.cache = cache\n        AARS.retry_count = retry_count if retry_count else 3\n\n    @classmethod\n    async def sync_indices(cls):\n        \"\"\"\n        Synchronizes all the indices created so far, by iteratively fetching all the messages from the channel,\n        having post_types of the Record subclasses that have been declared so far.\n        \"\"\"\n        for record in Record.__subclasses__():\n            if record.get_indices():\n                await record.regenerate_indices()\n\n    @classmethod\n    async def post_or_amend_object(cls,\n                                   obj: T,\n                                   account: Optional[str] = None,\n                                   channel: Optional[str] = None) -> T:\n        \"\"\"\n        Posts or amends an object to Aleph. If the object is already posted, it's list of revision hashes is updated and\n        the object receives the latest revision number.\n        :param obj: The object to post or amend.\n        :param account: The account to post the object with. If None, will use configured account.\n        :param channel: The channel to post the object to. If None, will use the configured channel.\n        :return: The object, as it is now on Aleph.\n        \"\"\"\n        if account is None:\n            account = cls.account\n        if channel is None:\n            channel = cls.channel\n        assert isinstance(obj, Record)\n        post_type = type(obj).__name__ if obj.id_hash is None else \"amend\"\n        resp = await client.create_post(account=account,\n                                        post_content=obj.content,\n                                        post_type=post_type,\n                                        channel=channel,\n                                        ref=obj.id_hash,\n                                        api_server=cls.api_url,\n                                        session=cls.session)\n        if obj.id_hash is None:\n            obj.id_hash = resp.item_hash\n        obj.revision_hashes.append(resp.item_hash)\n        obj.current_revision = len(obj.revision_hashes) - 1\n        if cls.cache:\n            await cls.cache.set(resp.item_hash, obj.json())\n        return obj\n\n    @classmethod\n    async def forget_objects(cls, objs: List[T], account: Optional[Account] = None, channel: Optional[str] = None):\n        \"\"\"\n        Forgets multiple objects from Aleph and local cache. All related revisions will be forgotten too.\n        :param objs: The objects to forget.\n        :param account: The account to delete the object with. If None, will use the fallback account.\n        :param channel: The channel to delete the object from. If None, will use the TEST channel of the object.\n        \"\"\"\n        if account is None:\n            account = cls.account\n        if channel is None:\n            channel = cls.channel\n        hashes = []\n        for obj in objs:\n            assert obj.id_hash is not None\n            hashes += [obj.id_hash] + obj.revision_hashes\n        forget_task = client.forget(account=account,\n                                    hashes=hashes,\n                                    reason=None,\n                                    channel=channel,\n                                    api_server=cls.api_url,\n                                    session=cls.session)\n        if cls.cache:\n            await asyncio.gather(\n                forget_task,\n                *[cls.cache.delete(h) for h in hashes]\n            )\n        else:\n            await forget_task\n\n    @classmethod\n    async def fetch_records(cls,\n                            datatype: Type[T],\n                            item_hashes: Optional[List[str]] = None,\n                            channel: Optional[str] = None,\n                            owner: Optional[str] = None) -> AsyncIterator[T]:\n        \"\"\"\n        Retrieves posts as objects by its aleph item_hash.\n\n        :param datatype: The type of the objects to retrieve.\n        :param item_hashes: Aleph item_hashes of the objects to fetch.\n        :param channel: Channel in which to look for it.\n        :param owner: Account that owns the object.\n        \"\"\"\n        assert issubclass(datatype, Record)\n        channels = None if channel is None else [channel]\n        owners = None if owner is None else [owner]\n        if item_hashes is None and channels is None and owners is None:\n            channels = [cls.channel]\n\n        if cls.cache and item_hashes is not None:\n            records = await cls._fetch_records_from_cache(datatype, item_hashes)\n            cached_ids = []\n            for r in records:\n                cached_ids.append(r.id_hash)\n                yield r\n            item_hashes = [h for h in item_hashes if h not in cached_ids]\n            if len(item_hashes) == 0:\n                return\n\n        async for record in cls._fetch_records_from_api(datatype=datatype,\n                                                        item_hashes=item_hashes,\n                                                        channels=channels,\n                                                        owners=owners):\n            yield record\n\n    @classmethod\n    async def _fetch_records_from_cache(cls, datatype: Type[T], item_hashes: List[str]) -> List[T]:\n        assert cls.cache\n        raw_records = await asyncio.gather(*[cls.cache.get(h) for h in item_hashes])\n        return [datatype.parse_raw(r) for r in raw_records if r is not None]\n\n    @classmethod\n    async def _fetch_records_from_api(cls,\n                                      datatype: Type[T],\n                                      item_hashes: Optional[List[str]] = None,\n                                      channels: Optional[List[str]] = None,\n                                      owners: Optional[List[str]] = None,\n                                      refs: Optional[List[str]] = None,\n                                      page=1) -> AsyncIterator[T]:\n        aleph_resp = None\n        retries = cls.retry_count\n        while aleph_resp is None:\n            try:\n                aleph_resp = await client.get_posts(hashes=item_hashes,\n                                                    channels=channels,\n                                                    types=[datatype.__name__],\n                                                    addresses=owners,\n                                                    refs=refs,\n                                                    api_server=cls.api_url,\n                                                    session=cls.session,\n                                                    pagination=50,\n                                                    page=page)\n            except ServerDisconnectedError:\n                retries -= 1\n                if retries == 0:\n                    raise\n        for post in aleph_resp['posts']:\n            yield await datatype.from_dict(post)\n\n        if page == 1:\n            # If there are more pages, fetch them\n            total_items = aleph_resp['pagination_total']\n            per_page = aleph_resp['pagination_per_page']\n            if total_items > per_page:\n                for next_page in range(2, math.ceil(total_items / per_page) + 1):\n                    async for record in cls._fetch_records_from_api(\n                            datatype=datatype,\n                            item_hashes=item_hashes,\n                            channels=channels,\n                            owners=owners,\n                            refs=refs,\n                            page=next_page):\n                        yield record\n\n    @classmethod\n    async def fetch_revisions(cls,\n                              datatype: Type[T],\n                              ref: str,\n                              channel: Optional[str] = None,\n                              owner: Optional[str] = None,\n                              page=1) -> AsyncIterator[str]:\n        \"\"\"Retrieves posts of revisions of an object by its item_hash.\n        :param datatype: The type of the objects to retrieve.\n        :param ref: item_hash of the object, whose revisions to fetch.\n        :param channel: Channel in which to look for it.\n        :param owner: Account that owns the object.\n        :param page: Page number to fetch.\"\"\"\n        owners = None if owner is None else [owner]\n        channels = None if channel is None else [channel]\n        if owners is None and channels is None:\n            channels = [cls.channel]\n\n        aleph_resp = None\n        retries = cls.retry_count\n        while aleph_resp is None:\n            try:\n                aleph_resp = await client.get_messages(channels=channels,\n                                                       addresses=owners,\n                                                       refs=[ref],\n                                                       api_server=cls.api_url,\n                                                       session=cls.session,\n                                                       pagination=50)\n            except ServerDisconnectedError:\n                retries -= 1\n                if retries == 0:\n                    raise\n        for message in aleph_resp.messages:\n            yield message.item_hash\n\n        if page == 1:\n            # If there are more pages, fetch them\n            total_items = aleph_resp.pagination_total\n            per_page = aleph_resp.pagination_per_page\n            if total_items > per_page:\n                for next_page in range(2, math.ceil(total_items / per_page) + 1):\n                    async for item_hash in cls.fetch_revisions(\n                            datatype=datatype,\n                            ref=ref,\n                            channel=channel,\n                            owner=owner,\n                            page=next_page):\n                        yield item_hash\n\n    @classmethod\n    async def fetch_exact(cls,\n                          datatype: Type[T],\n                          item_hash: str) -> T:\n        \"\"\"Retrieves the revision of an object by its item_hash of the message. The content will be exactly the same\n        as in the referenced message, so no amendments will be applied.\n\n        :param item_hash:\n        :param datatype: The type of the objects to retrieve.\n        \"\"\"\n        if cls.cache:\n            cache_resp = await cls._fetch_records_from_cache(datatype, [item_hash])\n            if len(cache_resp) > 0:\n                return cache_resp[0]\n        aleph_resp = await client.get_messages(hashes=[item_hash],\n                                               api_server=cls.api_url,\n                                               session=cls.session)\n        if len(aleph_resp.messages) == 0:\n            raise ValueError(f\"Message with hash {item_hash} not found.\")\n        message: PostMessage = aleph_resp.messages[0]\n        return await datatype.from_post(message)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/aars/core.py b/src/aars/core.py
--- a/src/aars/core.py	(revision 43a568824783a2f3051165268e106d0cbdf217bd)
+++ b/src/aars/core.py	(date 1675427702744)
@@ -61,7 +61,8 @@
         Updates the list of available revision hashes, in order to fetch these.
         """
         assert self.id_hash is not None
-        self.revision_hashes = [self.id_hash] + await async_iterator_to_list(AARS.fetch_revisions(type(self), ref=self.id_hash))
+        self.revision_hashes = [self.id_hash] + await async_iterator_to_list(
+            AARS.fetch_revisions(type(self), ref=self.id_hash))
         if self.current_revision is None:
             # latest revision
             self.current_revision = len(self.revision_hashes) - 1
@@ -175,7 +176,7 @@
     @classmethod
     async def fetch_all(cls: Type[T]) -> List[T]:
         """
-        Fetches all objects of given type.
+        Fetches all objects of given type. If called
         """
         return await async_iterator_to_list(AARS.fetch_records(cls))
 
@@ -318,7 +319,8 @@
         else:
             hashes = set()
 
-        return await async_iterator_to_list(AARS.fetch_records(self.datatype, list([h for h in hashes if h is not None])))
+        return await async_iterator_to_list(
+            AARS.fetch_records(self.datatype, list([h for h in hashes if h is not None])))
 
     def add_record(self, obj: T):
         """Adds a record to the index."""
@@ -343,11 +345,12 @@
 
     def __init__(self,
                  account: Optional[Account] = None,
-                 channel: Optional[str] = None,
+                 channel: str = 'AARS_TEST',
                  api_url: Optional[str] = None,
                  session: Optional[aiohttp.ClientSession] = None,
                  cache: Optional[VmCache] = None,
-                 retry_count: Optional[int] = None):
+                 retry_count: int = 3,
+                 sync_indices: bool = False):
         """
         Initializes the SDK with an account and a channel.
         :param cache: Whether to use Aleph VM caching when running AARS code.
@@ -357,11 +360,14 @@
         :param session: An aiohttp session to use. Defaults to a new session.
         """
         AARS.account = account if account else get_fallback_account()
-        AARS.channel = channel if channel else 'AARS_TEST'
         AARS.api_url = api_url if api_url else settings.API_HOST
-        AARS.session = session if session else None
+        AARS.channel = channel
+        AARS.session = session
         AARS.cache = cache
-        AARS.retry_count = retry_count if retry_count else 3
+        AARS.retry_count = retry_count
+
+        if sync_indices:
+            self.sync_indices()
 
     @classmethod
     async def sync_indices(cls):
@@ -439,26 +445,31 @@
 
     @classmethod
     async def fetch_records(cls,
-                            datatype: Type[T],
+                            datatypes: Union[Type[T], List[Type[T]]],
                             item_hashes: Optional[List[str]] = None,
                             channel: Optional[str] = None,
                             owner: Optional[str] = None) -> AsyncIterator[T]:
         """
         Retrieves posts as objects by its aleph item_hash.
 
-        :param datatype: The type of the objects to retrieve.
+        :param datatypes: The types of the objects to retrieve.
         :param item_hashes: Aleph item_hashes of the objects to fetch.
         :param channel: Channel in which to look for it.
         :param owner: Account that owns the object.
         """
-        assert issubclass(datatype, Record)
+        if not isinstance(datatypes, list):
+            datatypes = [datatypes]
+        datatypes_dict: Dict[str, Type[T]] = {}
+        for datatype in datatypes:
+            assert issubclass(datatype, Record)
+            datatypes_dict[datatype.__name__] = datatype
         channels = None if channel is None else [channel]
         owners = None if owner is None else [owner]
         if item_hashes is None and channels is None and owners is None:
             channels = [cls.channel]
 
         if cls.cache and item_hashes is not None:
-            records = await cls._fetch_records_from_cache(datatype, item_hashes)
+            records = await cls._fetch_records_from_cache(datatypes_dict, item_hashes)
             cached_ids = []
             for r in records:
                 cached_ids.append(r.id_hash)
@@ -467,7 +478,7 @@
             if len(item_hashes) == 0:
                 return
 
-        async for record in cls._fetch_records_from_api(datatype=datatype,
+        async for record in cls._fetch_records_from_api(datatypes=datatypes_dict,
                                                         item_hashes=item_hashes,
                                                         channels=channels,
                                                         owners=owners):
@@ -477,23 +488,24 @@
     async def _fetch_records_from_cache(cls, datatype: Type[T], item_hashes: List[str]) -> List[T]:
         assert cls.cache
         raw_records = await asyncio.gather(*[cls.cache.get(h) for h in item_hashes])
-        return [datatype.parse_raw(r) for r in raw_records if r is not None]
+        return [datatype.parse_raw(r) for r in raw_records if r is not None and not isinstance(r, BaseException)]
 
     @classmethod
     async def _fetch_records_from_api(cls,
-                                      datatype: Type[T],
+                                      datatypes: Dict[str, Type[T]],
                                       item_hashes: Optional[List[str]] = None,
                                       channels: Optional[List[str]] = None,
                                       owners: Optional[List[str]] = None,
                                       refs: Optional[List[str]] = None,
                                       page=1) -> AsyncIterator[T]:
+        types = [datatype.__name__ for datatype in datatypes.values()]
         aleph_resp = None
         retries = cls.retry_count
         while aleph_resp is None:
             try:
                 aleph_resp = await client.get_posts(hashes=item_hashes,
                                                     channels=channels,
-                                                    types=[datatype.__name__],
+                                                    types=types,
                                                     addresses=owners,
                                                     refs=refs,
                                                     api_server=cls.api_url,
@@ -505,6 +517,7 @@
                 if retries == 0:
                     raise
         for post in aleph_resp['posts']:
+            datatype = datatypes[post['content']['type']]
             yield await datatype.from_dict(post)
 
         if page == 1:
@@ -514,7 +527,7 @@
             if total_items > per_page:
                 for next_page in range(2, math.ceil(total_items / per_page) + 1):
                     async for record in cls._fetch_records_from_api(
-                            datatype=datatype,
+                            datatypes=datatypes,
                             item_hashes=item_hashes,
                             channels=channels,
                             owners=owners,
